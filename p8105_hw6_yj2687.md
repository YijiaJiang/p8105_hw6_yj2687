p8105_hw6_yj2687
================
Yijia Jiang
2022-12-02

## Problem 1 (2017 Central Park Weather Dataset)

``` r
# Import the dataset
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_yj2687_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

The $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause
for the generally skewed shape of the distribution. If we wanted to
construct a confidence interval for $r^2$, we could take the 2.5% and
97.5% quantiles of the estimates across bootstrap samples. However,
because the shape isn’t symmetric, using the mean +/- 1.96 times the
standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_yj2687_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

 

## Problem 2 (Homicide Dataset)

``` r
# Import the dataset
homicide_raw <- read_csv(url("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"), na = c("", "NA", "Unknown"))

# Clean dataset
# Create variables city_state, resolved;
# Omit cities without victim race reports and error city;
# Limit victim_race to white or black
homicide_df = homicide_raw %>% 
  janitor::clean_names() %>%
  mutate(reported_date = as.Date(as.character(reported_date), format = "%Y%m%d"),
         victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White"),
         city_state = str_c(city, state, sep = ", "),
         resolved = case_when(
           disposition == "Closed without arrest" ~ 0,
           disposition == "Open/No arrest" ~ 0,
           disposition == "Closed by arrest" ~ 1)) %>% 
  relocate(city_state) %>% 
  filter(!city_state %in% c("Dallas, TX","Phoenix, AZ","Kansas City, MO","Tulsa, AL"),
         victim_race %in% c("White","Black"))
```

``` r
#  For the city of Baltimore, MD, fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.
baltimore_logistic = homicide_df %>%
    filter(city_state == "Baltimore, MD") %>% 
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial(link = "logit")) 

# Obtain the estimate and CI of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
baltimore_logistic %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(exp(conf.low)),
         CI_upper = exp(exp(conf.high)),
         p_val = rstatix::p_format(p.value, digits = 2)) %>% 
  select(term, OR, CI_lower,CI_upper, p_val) %>% 
  mutate(term = str_replace(term, "victim_age", "Victim age"),
         term = str_replace(term, "victim_race", "Victim Race: "),
         term = str_replace(term, "victim_sex", "Victim Sex: ")) %>% 
  knitr::kable(digits = 3, align = "lccc", 
               col.names = c("Term", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

| Term               | Estimated adjusted OR | CI lower bound | CI upper bound | p-value  |
|:-------------------|:---------------------:|:--------------:|:--------------:|:---------|
| (Intercept)        |         3.164         |     7.372      |    157.182     | \<0.0001 |
| Victim age         |         0.993         |     2.683      |     2.718      | 0.043    |
| Victim Race: Black |         0.431         |     1.357      |     1.833      | \<0.0001 |
| Victim Sex: Male   |         0.426         |     1.383      |     1.746      | \<0.0001 |

-   For the city of Baltimore, MD, controlling for all other variables,
    the homicides whose victim is male are significantly less like to be
    resolved than those whose victim is female.

``` r
# For all the cities, extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims
allcities_logistic = homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial(link = "logit"))),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(conf.low),
    CI_upper = exp(conf.high),
    p_val = rstatix::p_format(p.value, digits = 2)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, OR, CI_lower,CI_upper, p_val) 

allcities_logistic %>% 
  knitr::kable(digits = 3, align = "llccc", col.names = c("City", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

| City               | Estimated adjusted OR | CI lower bound | CI upper bound | p-value  |
|:-------------------|:----------------------|:--------------:|:--------------:|:--------:|
| Albuquerque, NM    | 1.767                 |     0.825      |     3.762      |  0.1393  |
| Atlanta, GA        | 1.000                 |     0.680      |     1.458      | 0.99968  |
| Baltimore, MD      | 0.426                 |     0.324      |     0.558      | \<0.0001 |
| Baton Rouge, LA    | 0.381                 |     0.204      |     0.684      | 0.00165  |
| Birmingham, AL     | 0.870                 |     0.571      |     1.314      | 0.51115  |
| Boston, MA         | 0.667                 |     0.351      |     1.260      | 0.21213  |
| Buffalo, NY        | 0.521                 |     0.288      |     0.936      | 0.02895  |
| Charlotte, NC      | 0.884                 |     0.551      |     1.391      | 0.60041  |
| Chicago, IL        | 0.410                 |     0.336      |     0.501      | \<0.0001 |
| Cincinnati, OH     | 0.400                 |     0.231      |     0.667      | 0.00065  |
| Columbus, OH       | 0.532                 |     0.377      |     0.748      |  0.0003  |
| Denver, CO         | 0.479                 |     0.233      |     0.962      |  0.0411  |
| Detroit, MI        | 0.582                 |     0.462      |     0.734      | \<0.0001 |
| Durham, NC         | 0.812                 |     0.382      |     1.658      | 0.57611  |
| Fort Worth, TX     | 0.669                 |     0.394      |     1.121      | 0.13117  |
| Fresno, CA         | 1.335                 |     0.567      |     3.048      | 0.49638  |
| Houston, TX        | 0.711                 |     0.557      |     0.906      | 0.00593  |
| Indianapolis, IN   | 0.919                 |     0.678      |     1.241      | 0.58189  |
| Jacksonville, FL   | 0.720                 |     0.536      |     0.965      | 0.02832  |
| Las Vegas, NV      | 0.837                 |     0.606      |     1.151      | 0.27761  |
| Long Beach, CA     | 0.410                 |     0.143      |     1.024      | 0.07176  |
| Los Angeles, CA    | 0.662                 |     0.457      |     0.954      | 0.02793  |
| Louisville, KY     | 0.491                 |     0.301      |     0.784      | 0.00337  |
| Memphis, TN        | 0.723                 |     0.526      |     0.984      | 0.04205  |
| Miami, FL          | 0.515                 |     0.304      |     0.873      | 0.01348  |
| Milwaukee, wI      | 0.727                 |     0.495      |     1.054      | 0.09767  |
| Minneapolis, MN    | 0.947                 |     0.476      |     1.881      | 0.87573  |
| Nashville, TN      | 1.034                 |     0.681      |     1.556      | 0.87289  |
| New Orleans, LA    | 0.585                 |     0.422      |     0.812      | 0.00131  |
| New York, NY       | 0.262                 |     0.133      |     0.485      | \<0.0001 |
| Oakland, CA        | 0.563                 |     0.364      |     0.867      | 0.00937  |
| Oklahoma City, OK  | 0.974                 |     0.623      |     1.520      | 0.90794  |
| Omaha, NE          | 0.382                 |     0.199      |     0.711      | 0.00295  |
| Philadelphia, PA   | 0.496                 |     0.376      |     0.650      | \<0.0001 |
| Pittsburgh, PA     | 0.431                 |     0.263      |     0.696      | 0.00067  |
| Richmond, VA       | 1.006                 |     0.483      |     1.994      | 0.98658  |
| San Antonio, TX    | 0.705                 |     0.393      |     1.238      | 0.23034  |
| Sacramento, CA     | 0.669                 |     0.326      |     1.314      | 0.25481  |
| Savannah, GA       | 0.867                 |     0.419      |     1.780      | 0.69735  |
| San Bernardino, CA | 0.500                 |     0.166      |     1.462      | 0.20567  |
| San Diego, CA      | 0.413                 |     0.191      |     0.830      | 0.01722  |
| San Francisco, CA  | 0.608                 |     0.312      |     1.155      | 0.13362  |
| St. Louis, MO      | 0.703                 |     0.530      |     0.932      | 0.01439  |
| Stockton, CA       | 1.352                 |     0.626      |     2.994      | 0.44745  |
| Tampa, FL          | 0.808                 |     0.340      |     1.860      | 0.61939  |
| Tulsa, OK          | 0.976                 |     0.609      |     1.544      | 0.91746  |
| Washington, DC     | 0.691                 |     0.466      |     1.014      | 0.06167  |

``` r
# Create a plot showing the estimated ORs and CIs for each city
allcities_logistic %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = "City", y = "Estimated OR with CI")
```

<img src="p8105_hw6_yj2687_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

-    

## Problem 3 (Child’s birthweight Dataset)

``` r
# Import the dataset
birthweight_raw <- read_csv("./p8105_hw6_data/birthweight.csv")

# Tidy the dataset
```
