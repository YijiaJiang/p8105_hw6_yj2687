---
title: "p8105_hw6_yj2687"
author: "Yijia Jiang"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(purrr)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1 (2017 Central Park Weather Dataset)

```{r, warning=FALSE, message=FALSE}
# Import the dataset
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r, warning=FALSE, message=FALSE}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

The $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $r^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.
```{r, warning=FALSE, message=FALSE}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


&nbsp;


## Problem 2 (Homicide Dataset)

```{r, warning=FALSE, message=FALSE}
# Import the dataset
homicide_raw <- read_csv(url("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"))

# Clean dataset
# Create variables city_state, resolved;
# Omit cities without victim race reports and error city;
# Limit victim_race to white or black
homicide_df = homicide_raw %>% 
  janitor::clean_names() %>%
  mutate(reported_date = as.Date(as.character(reported_date), format = "%Y%m%d"),
         victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White"),
         city_state = str_c(city, state, sep = ", "),
         resolved = case_when(
           disposition == "Closed without arrest" ~ 0,
           disposition == "Open/No arrest" ~ 0,
           disposition == "Closed by arrest" ~ 1)) %>% 
  relocate(city_state) %>% 
  filter(!city_state %in% c("Dallas, TX","Phoenix, AZ","Kansas City, MO","Tulsa, AL"),
         victim_race %in% c("White","Black"))
```



```{r, warning=FALSE, message=FALSE}
#  For the city of Baltimore, MD, fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.
baltimore_logistic = homicide_df %>%
    filter(city_state == "Baltimore, MD") %>% 
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial(link = "logit")) 

# Obtain the estimate and CI of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
baltimore_logistic %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(exp(conf.low)),
         CI_upper = exp(exp(conf.high)),
         p_val = rstatix::p_format(p.value, digits = 4)) %>% 
  select(term, OR, CI_lower,CI_upper, p_val) %>% 
  mutate(term = str_replace(term, "victim_age", "Victim age"),
         term = str_replace(term, "victim_race", "Victim Race: "),
         term = str_replace(term, "victim_sex", "Victim Sex: ")) %>% 
  knitr::kable(digits = 3, align = "lccc", 
               col.names = c("Term", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

* For the city of Baltimore, MD, controlling for all other variables, the homicides whose victim is male are significantly less like to be resolved than those whose victim is female.


```{r, warning=FALSE, message=FALSE}
# For all the cities, extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims
allcities_logistic = homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial(link = "logit"))),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(conf.low),
    CI_upper = exp(conf.high),
    p_val = rstatix::p_format(p.value, digits = 2)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, OR, CI_lower,CI_upper, p_val) 

allcities_logistic %>% 
  knitr::kable(digits = 3, align = "llccc", col.names = c("City", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```


```{r, warning=FALSE, message=FALSE}
# Create a plot showing the estimated ORs and CIs for each city
allcities_logistic %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = "City", y = "Estimated OR with CI",
       title = "Odds Ratio for Solving Homicides Comparing Male to Female Victims")
```

* 
&nbsp;


## Problem 3 (Childâ€™s birthweight Dataset)

```{r, warning=FALSE, message=FALSE}
# Import the dataset
birthweight_raw <- read_csv("./p8105_hw6_data/birthweight.csv")

# Tidy the dataset
birthweight_df = birthweight_raw %>% 
  janitor::clean_names() %>%
  mutate(across(.cols = c(babysex, malform, frace, mrace), as.factor))

# Check for NA
map(birthweight_df, ~sum(is.na(.)))

# Dataset summary
skimr::skim(birthweight_df)
```

* In the data tidying and wrangling process, I converted the variable `babysex`, `malform`, `frace`, and `mrace` into factors as they are categorical variables.
* In our tidied dataset, no missing values was detected in all variables.

```{r, warning=FALSE, message=FALSE}
# Propose a regression model for birthweight
# Based on a data-driven model-building process
mult.fit = lm(bwt ~ ., data = birthweight_df)
step(mult.fit, direction = "both", k = 2)
```

* By using the stepwise regression with Akaike information criterion (AIC), it determines objectively the best model as the one that minimizes the considered information criterion. The resulting model has 11 variables, which are `babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt` and `smoken`.

```{r, warning=FALSE, message=FALSE}
model_fit1 = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight_df)
summary(model_fit1)
```

* After looking at the summary, I found out the p-value of `fincome` is greater than 0.05 and hence I am going to drop it.

```{r, warning=FALSE, message=FALSE}
# Check correlation between predictors and the selected continuous variables
birthweight_df %>%
  select(bhead, blength, delwt, fincome, gaweeks, mheight, parity, ppwt, smoken) %>% 
  PerformanceAnalytics::chart.Correlation(method = "pearson")
```

* According to the correlation plot above, we can identify a potential collinearity between `delwt` and `ppwt` and between `bhead` and `blength`. Therefore, I plan to exclude `ppwt` and `blength` from my model, whose p-values are relatively larger in my model.

```{r, warning=FALSE, message=FALSE}
# Model after dropping blength, ppwt and fincome
model_fit2 = lm(bwt ~ babysex + bhead + delwt + gaweeks + mheight + mrace + parity + smoken, data = birthweight_df)
summary(model_fit2)
```

* After dropping the three variables `blength`, `ppwt` and `fincome`, I rebuild my model and obtain the results that the p-value for `parity` is greater than 0.05, which is not signicant at the level of 0.05. so I drop this variable too.

```{r, warning=FALSE, message=FALSE}
# Model after dropping blength, ppwt, fincome and parity
model_fit3 = lm(bwt ~ babysex + bhead + delwt + gaweeks + mheight + mrace + smoken, data = birthweight_df)
summary(model_fit3)
```

* Final model is as follows: bwt ~  babysex + bhead + delwt + gaweeks + mheight + mrace + smoken.

```{r, warning=FALSE, message=FALSE}
# Make a plot of model residuals against fitted values
birthweight_df %>%
  add_residuals(model_fit3) %>%
  add_predictions(model_fit3) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Fitted Values", y = "Residuals", 
       title = "Model Residuals Against Fitted Values") 
```


```{r, warning=FALSE, message=FALSE}
# Model using length at birth and gestational age as predictors (main effects only)
comp_fit1 = lm(bwt ~ blength + gaweeks, data = birthweight_df)

# Model using head circumference, length, sex, and all interactions (including the three-way interaction) between these
comp_fit2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_df)
```



```{r, warning=FALSE, message=FALSE}
# Make a comparison with two other models in terms of the cross-validated prediction error
# Cross Validation
cv_df =
  crossv_mc(birthweight_df, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(model1 = map(train, ~lm(bwt ~ babysex + bhead + delwt + gaweeks + mheight + mrace + smoken, data = as_tibble(.x))),
         model2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(x = "Model", y = "Root Mean Square Error (RMSE)",
       title = "Model Comparison of the Cross-Validated Prediction Error") +
  scale_x_discrete(labels = c("My model", "Model with Main effects", "Model with Interactions"))

